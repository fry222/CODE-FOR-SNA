{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import os\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "from tqdm import tqdm \n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from spacy.pipeline import EntityRuler\n",
    "from shapely.geometry import Point, LineString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_trf\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the cleaned data and get an overview\n",
    "docs = []\n",
    "\n",
    "directory= 'data4/clean/'\n",
    "\n",
    "whole = \"\"\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.txt'):  \n",
    "        file_path = os.path.join(directory, filename)\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "            whole += content + \"\\n\\n\"  \n",
    "\n",
    "\n",
    "print(whole)  \n",
    "nlp.max_length = len(whole) + 100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the street names in Manchester extrated from GB1900   \n",
    "file_path = './manchester_data.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "\n",
    "street_names = df['final_text'].dropna().unique()\n",
    "\n",
    "\n",
    "whole_text = whole\n",
    "\n",
    "\n",
    "def is_valid_street_name(name):\n",
    "    if len(name) > 5 and len(name) < 50:\n",
    "        if re.match(\"^[A-Za-z0-9 .,'&-]+$\", name):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "valid_street_names = [name for name in street_names if is_valid_street_name(name)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build the network of George Thompson with the stree names in GB1900 and person's names\n",
    "exclude_streets = set(['Library', 'Temple'])\n",
    "valid_street_set = {street for street in valid_street_names if street not in exclude_streets}\n",
    "\n",
    "directory = 'data9/clean/'\n",
    "G = nx.Graph() \n",
    "\n",
    "street_person_map = {}\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.txt'):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "            paragraphs = content.split('\\n\\n')\n",
    "\n",
    "            for paragraph_text in paragraphs:\n",
    "                paragraph_doc = nlp(paragraph_text)\n",
    "                sents = list(paragraph_doc.sents)\n",
    "\n",
    "                local_streets = {}\n",
    "                for ent in paragraph_doc.ents:\n",
    "                    if ent.text in valid_street_set:\n",
    "                        if ent.text not in G:\n",
    "                            G.add_node(ent.text, type='street')\n",
    "                        if ent.text not in street_person_map:\n",
    "                            street_person_map[ent.text] = set()\n",
    "                        local_streets[ent.text] = set()\n",
    "\n",
    "                for i, sent in enumerate(sents):\n",
    "                    if \"george thompson\" in sent.text.lower():\n",
    "                        start_index = max(0, i - 3)\n",
    "                        end_index = min(len(sents), i + 4)\n",
    "                        for j in range(start_index, end_index):\n",
    "                            for ent in sents[j].ents:\n",
    "                                if ent.label_ == 'PERSON' and ent.text != \"George Thompson\":\n",
    "                                    for street in local_streets.keys():\n",
    "                                        local_streets[street].add(ent.text)\n",
    "\n",
    "                \n",
    "                for street, persons in local_streets.items():\n",
    "                    street_person_map[street].update(persons)\n",
    "\n",
    "\n",
    "for street1, people1 in street_person_map.items():\n",
    "    for street2, people2 in street_person_map.items():\n",
    "        if street1 != street2:\n",
    "            shared_people_count = len(people1.intersection(people2))\n",
    "            if shared_people_count > 0:\n",
    "                if G.has_edge(street1, street2):\n",
    "                    G[street1][street2]['weight'] += shared_people_count\n",
    "                else:\n",
    "                    G.add_edge(street1, street2, weight=shared_people_count)\n",
    "\n",
    "print(f\"Number of nodes: {G.number_of_nodes()}\")\n",
    "print(f\"Number of edges: {G.number_of_edges()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#have an overview of the network\n",
    "output_path = 'georgelocation.gexf'\n",
    "nx.write_gexf(G, output_path)\n",
    "print(f\"Graph has been saved to {output_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add coordinate in GB1900 into the nodes\n",
    "file_path = './manchester_data.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "for index, row in data.iterrows():\n",
    "    street_name = row['final_text'] \n",
    "    if street_name in G:\n",
    "        \n",
    "        G.nodes[street_name]['latitude'] = row['latitude']\n",
    "        G.nodes[street_name]['longitude'] = row['longitude']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the degree_centrality and the weight of the edges\n",
    "degree_centrality = nx.degree_centrality(G)\n",
    "nx.set_node_attributes(G, degree_centrality, 'degree_centrality')\n",
    "\n",
    "\n",
    "node_data = {\n",
    "    'geometry': [],\n",
    "    'size': [],\n",
    "    'degree_centrality': []  \n",
    "}\n",
    "for node, attr in G.nodes(data=True):\n",
    "    if 'latitude' in attr and 'longitude' in attr:\n",
    "        point = Point(attr['longitude'], attr['latitude'])\n",
    "        node_data['geometry'].append(point)\n",
    "        node_data['size'].append(attr.get('size', 1))  \n",
    "        node_data['degree_centrality'].append(attr.get('degree_centrality', 0)) \n",
    "\n",
    "nodes_gdf = gpd.GeoDataFrame(node_data, crs=\"EPSG:4326\")\n",
    "\n",
    "\n",
    "edge_data = {\n",
    "    'geometry': [],\n",
    "    'weight': []\n",
    "}\n",
    "for u, v, data in G.edges(data=True):\n",
    "    if 'latitude' in G.nodes[u] and 'longitude' in G.nodes[u] and 'latitude' in G.nodes[v] and 'longitude' in G.nodes[v]:\n",
    "        line = LineString([(G.nodes[u]['longitude'], G.nodes[u]['latitude']),\n",
    "                           (G.nodes[v]['longitude'], G.nodes[v]['latitude'])])\n",
    "        edge_data['geometry'].append(line)\n",
    "        edge_data['weight'].append(data.get('weight', 1))  \n",
    "\n",
    "edges_gdf = gpd.GeoDataFrame(edge_data, crs=\"EPSG:4326\")\n",
    "\n",
    "\n",
    "output_path = './ditu/' \n",
    "\n",
    "nodes_gdf.to_file(f\"{output_path}network_nodes1.shp\")\n",
    "edges_gdf.to_file(f\"{output_path}network_edges1.shp\")\n",
    "\n",
    "print(\"Shapefiles have been saved successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
